{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.label = None\n",
    "        self.split_rule = None\n",
    "        self.height = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, MAX_HEIGHT, NODE_PURITY, INFORMATION_GAIN, features_random = 32): #X.shape[1] = 32\n",
    "        \"\"\"\n",
    "        initialization of a decision tree\n",
    "        \"\"\"\n",
    "        self.Node = Node()\n",
    "        self.MAX_HEIGHT = MAX_HEIGHT\n",
    "        self.NODE_PURITY = NODE_PURITY\n",
    "        self.INFORMATION_GAIN = INFORMATION_GAIN\n",
    "        self.features_random = features_random\n",
    "        \n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\"\n",
    "        entropy given all the labels\n",
    "        \"\"\"\n",
    "        if len(y)==0:\n",
    "            return 0\n",
    "        p_1 = sum(y)/len(y)\n",
    "        p_0 = 1 - p_1\n",
    "        \n",
    "        if p_1 == 0:\n",
    "            A = 0\n",
    "        else:\n",
    "            A = -p_1*np.log10(p_1)\n",
    "        if p_0 == 0:\n",
    "            B = 0\n",
    "        else:\n",
    "            B = - p_0*np.log10(p_0)\n",
    "        H = A+B\n",
    "        return H\n",
    "\n",
    "    @staticmethod\n",
    "    def information_gain(X, y, thresh):\n",
    "        \"\"\"\n",
    "        information gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        T = np.array([(x<thresh) for x in X]).T\n",
    "        F = T.copy()\n",
    "        T = True^T\n",
    "        y_T = np.ma.masked_array(y, T).compressed()\n",
    "        y_F = np.ma.masked_array(y, F).compressed()\n",
    "        T = 1 - sum(T)/len(T)\n",
    "        F = 1 - T\n",
    "        H = T*DecisionTree.entropy(y_T)+F*DecisionTree.entropy(y_F)\n",
    "        IG = DecisionTree.entropy(y) - H\n",
    "        return IG\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_impurity(y):\n",
    "        \"\"\"\n",
    "        gini impurity given all the labels\n",
    "        \"\"\"\n",
    "        if len(y)==0:\n",
    "            return 0\n",
    "        p_1 = sum(y)/len(y)\n",
    "        p_0 = 1 - p_1\n",
    "        return 1 - p_1**2 - p_0**2\n",
    "\n",
    "    @staticmethod\n",
    "    def gini_purification(X, y, thresh):\n",
    "        \"\"\"\n",
    "        calculates reduction in impurity gain given a vector of features\n",
    "        and a split threshold\n",
    "        \"\"\"\n",
    "        T = np.array([(x<thresh) for x in X]).T\n",
    "        F = T.copy()\n",
    "        T = True^T\n",
    "        y_T = np.ma.masked_array(y, T).compressed()\n",
    "        y_F = np.ma.masked_array(y, F).compressed()\n",
    "        T = 1 - sum(T)/len(T)\n",
    "        F = 1 - T\n",
    "        G = T*DecisionTree.gini_impurity(y_T)+F*DecisionTree.gini_impurity(y_F)\n",
    "        return DecisionTree.gini_impurity(y) - G\n",
    "\n",
    "    def split(self, X, y, idx, thresh):\n",
    "        \"\"\"\n",
    "        return a split of the dataset given an index of the feature and\n",
    "        a threshold for it\n",
    "        \"\"\"\n",
    "        T = np.array([(x<thresh) for x in X[:,idx]])\n",
    "        T = np.array([T for _ in range(X.shape[1])]).T\n",
    "        F = T.copy()\n",
    "        T = True^T\n",
    "\n",
    "        X_A = np.ma.compress_rows(np.ma.masked_array(X, T))\n",
    "        X_B = np.ma.compress_rows(np.ma.masked_array(X, F))\n",
    "        y_A = np.ma.masked_array(y, T[:,0]).compressed()\n",
    "        y_B = np.ma.masked_array(y, F[:,0]).compressed()\n",
    "        return [[X_A, y_A],[X_B, y_B]]\n",
    "    \n",
    "    def segmenter(self, X, y):\n",
    "        \"\"\"\n",
    "        compute entropy gain for all single-dimension splits,\n",
    "        return the feature and the threshold for the split that\n",
    "        has maximum gain\n",
    "        \"\"\"\n",
    "        features = np.random.choice(X.shape[1], self.features_random, replace = False)\n",
    "            \n",
    "        gain = np.zeros((X.shape))\n",
    "        for j in features:\n",
    "            unique = np.unique(X[:,j])\n",
    "            gain_unique = dict()\n",
    "            for i in range(len(unique)):\n",
    "                thresh = unique[i]\n",
    "                gain_unique[unique[i]] = DecisionTree.information_gain(X[:,j],y,thresh)\n",
    "            for i in range(1,X.shape[0]):\n",
    "                gain[i,j] = gain_unique[X[i,j]]\n",
    "        idx = np.unravel_index(np.argmax(gain, axis=None), gain.shape)\n",
    "        return [idx[1], X[idx[0], idx[1]]] \n",
    "    \n",
    "    \n",
    "    def train(self, X, y, node=None):\n",
    "        \"\"\"\n",
    "        fit the model to a training set.\n",
    "        \"\"\"\n",
    "        if node == None:\n",
    "            self.Node = Node()\n",
    "            node = self.Node\n",
    "        if node.height > self.MAX_HEIGHT or np.sum(y)/len(y) > self.NODE_PURITY or np.sum(y)/len(y) < 1-self.NODE_PURITY or len(X)<=1:\n",
    "            node.label = int(np.sum(y)/len(y) >= 0.5)\n",
    "        else:\n",
    "            idx, thresh = self.segmenter(X,y)\n",
    "            A, B = self.split(X,y,idx,thresh)\n",
    "            X_A, y_A = A\n",
    "            X_B, y_B = B\n",
    "            IG = DecisionTree.information_gain(X[:,idx], y, thresh)\n",
    "            if IG >= self.INFORMATION_GAIN and len(y_A)>0 and len(y_B)>0:\n",
    "                node.split_rule = [idx, thresh]\n",
    "                \n",
    "                \n",
    "                node.left = Node()\n",
    "                node.left.height = node.height + 1\n",
    "                self.train(X_A, y_A, node.left)\n",
    "                node.right = Node()\n",
    "                node.right.height = node.height + 1\n",
    "                self.train(X_B, y_B, node.right)\n",
    "            else:\n",
    "                node.label = int(np.sum(y)/len(y) >= 0.5)   \n",
    "\n",
    "    def predict_single(self, X, node=None):\n",
    "        \"\"\"\n",
    "        predict the labels for input data \n",
    "        \"\"\"\n",
    "        if node == None:\n",
    "            node = self.Node\n",
    "        if node.label != None:\n",
    "            #print('Therefore, this email is: ', class_names[node.label]+'\\n')\n",
    "            return node.label\n",
    "        else:\n",
    "            idx, thresh = node.split_rule\n",
    "            if X[idx] < thresh and node.left != None:\n",
    "                #print(features[idx] + ' < ' + str(thresh)+'\\n')\n",
    "                return self.predict_single(X, node.left)\n",
    "            elif X[idx] >= thresh and node.right != None:\n",
    "                #print(features[idx] + ' >= ' + str(thresh)+'\\n')\n",
    "                return self.predict_single(X, node.right)\n",
    "                \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self.predict_single(x))\n",
    "        return predictions\n",
    "\n",
    "    def __repr__(self, node=None, indent=0):\n",
    "        \"\"\"\n",
    "        String representation of the tree\n",
    "        \"\"\"\n",
    "        if node == None:\n",
    "            node = self.Node\n",
    "        if node.label == None:\n",
    "            idx, thresh = node.split_rule\n",
    "            print('  '*indent + str(features[idx])+': '+'x < '+str(thresh))\n",
    "            if node.left!=None:\n",
    "                self.__repr__(node.left, indent + 1)\n",
    "            if node.right!=None:\n",
    "                self.__repr__(node.right, indent + 1)\n",
    "        else:\n",
    "            print('  '*indent + 'label: ' + str(class_names[node.label]))\n",
    "            \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = [y_test[i] == predictions[i] for i in range(len(y_test))]\n",
    "        if len(accuracy)==0:\n",
    "            return 0\n",
    "        return sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using the Decision Tree class to implement a Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    def __init__(self, MAX_HEIGHT, NODE_PURITY, INFORMATION_GAIN, features_randomization, number_of_trees, subsample):\n",
    "        \"\"\"\n",
    "        TODO: initialization of a random forest\n",
    "        \"\"\"\n",
    "        self.subsample = subsample\n",
    "        self.forest = [DecisionTree(MAX_HEIGHT, NODE_PURITY, INFORMATION_GAIN, features_randomization) for _ in range(number_of_trees)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        TODO: fit the model to a training set.\n",
    "        \"\"\"\n",
    "        for tree in self.forest:\n",
    "            subsample = np.random.choice(len(X), self.subsample)\n",
    "            X_train = np.array([X[i] for i in subsample])\n",
    "            y_train = np.array([y[i] for i in subsample])\n",
    "            tree.train(X_train, y_train)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        TODO: predict the labels for input data \n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for tree in self.forest:\n",
    "            predictions.append(tree.predict(X))\n",
    "        predictions = np.array(predictions).T\n",
    "        return np.array([(sum(pred)/len(pred) >= 0.5) for pred in predictions])\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        accuracy = [y_test[i] == predictions[i] for i in range(len(y_test))]\n",
    "        if len(accuracy)==0:\n",
    "            return 0\n",
    "        return sum(accuracy)/len(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementing k-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(model, X, y, k=10):\n",
    "    training_acc = 0\n",
    "    testing_acc = 0\n",
    "    cpt = 0\n",
    "    step = len(X)//k\n",
    "    for i in range(0,k):\n",
    "        if i==k-1:\n",
    "            X_test = X[i*step::]\n",
    "            y_test = y[i*step::]\n",
    "            X_train = X[0:i*step]\n",
    "            y_train = y[0:i*step]\n",
    "        else:\n",
    "            X_test = X[i*step:(i+1)*step]\n",
    "            y_test = y[i*step:(i+1)*step]\n",
    "            X_train = np.concatenate((X[0:i*step],X[(i+1)*step::]))\n",
    "            y_train = np.concatenate((y[0:i*step],y[(i+1)*step::]))\n",
    "        \n",
    "        model.train(X_train, y_train)\n",
    "        model_train_acc = model.evaluate(X_train, y_train)\n",
    "        training_acc += model_train_acc\n",
    "        model_test_acc = model.evaluate(X_test, y_test)\n",
    "        testing_acc += model_test_acc\n",
    "        print('Training {} of {}: - Training accuracy: {} - Testing accuracy: {}'.format(i+1, k, model_train_acc, model_test_acc))\n",
    "    print('\\nAverage Training Accuracy: {} / Average Testing Accuracy: {}'.format(training_acc/k, testing_acc/k))\n",
    "    return training_acc/k, testing_acc/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a dataset, one can plot the influence of the depth of a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for i in range(1,40):\n",
    "        depth.append(i)\n",
    "        classifier = DecisionTree(i, 1, 0.0)\n",
    "        classifier.train(X_train, y_train)\n",
    "        train = classifier.evaluate(X_train, y_train)\n",
    "        test = classifier.evaluate(X_test, y_test)\n",
    "        training_acc.append(train)\n",
    "        testing_acc.append(test)\n",
    "    plt.plot(depth, testing_acc, label='Validation Accuracy')\n",
    "    plt.plot(depth, training_acc, label='Training Accuracy')\n",
    "    plt.xlabel('Depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy as a Function of the Depth')\n",
    "    plt.legend()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training and Visualizing a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTree(5, 1, 0.0)\n",
    "training_accuracy, testing_accuracy = k_fold_cv(classifier, X, y, 5) #to get an idea of the classifier's accuracy\n",
    "\n",
    "classifier.train(X,y)\n",
    "classifier.__repr__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
